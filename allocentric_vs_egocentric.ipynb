{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Egocentric vs. allocentric pose\n",
    "\n",
    "For the task of pose estimation often the allocentric, instead of the egocentric pose is used.\n",
    "This is, because when using the egocentric pose different visual features need to be mapped onto the same rotation (a).\n",
    "Using the allocentric pose alleviates this problem, as visual features do not vary that much (b).\n",
    "\n",
    "![](https://miro.medium.com/max/720/1*9ngEGXpc_nemUxBYZSE8Ww.png)\n",
    "\n",
    "Image from [Deep Fitting Degree Scoring Network for Monocular 3D Object Detection](https://arxiv.org/pdf/1904.12681.pdf), found online at https://miro.medium.com/max/720/1*9ngEGXpc_nemUxBYZSE8Ww.png.\n",
    "\n",
    "<br>\n",
    "\n",
    "References:\n",
    "- [3D-RCNN: Instance-level 3D Object Reconstruction via Render-and-Compare](https://openaccess.thecvf.com/content_cvpr_2018/papers/Kundu_3D-RCNN_Instance-Level_3D_CVPR_2018_paper.pdf) \n",
    "- [Orientation Estimation in Monocular 3D Object Detection](https://towardsdatascience.com/orientation-estimation-in-monocular-3d-object-detection-f850ace91411)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparation\n",
    "\n",
    "In the following we define some utility functions. You can skip this part if you just want to fiddle with the visualization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# When running in Colab\n",
    "# %pip install transforms3d trimesh git+https://github.com/skoch9/meshplot.git pythreejs==2.0.0 matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import meshplot as mp\n",
    "import numpy as np\n",
    "import trimesh\n",
    "from IPython.display import display, HTML, display_html\n",
    "from scipy.spatial.transform import Rotation as R\n",
    "from transforms3d.affines import compose\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mp.website()\n",
    "\n",
    "def visualize3d(points: List[dict] = None, meshes: List[dict] = None, lines: List[dict] = None):\n",
    "    \"\"\"\n",
    "    Visualize a set of points, lines and meshes. The dictionaries will be passed onto meshplot and need to contain the\n",
    "    correct plot configuration. \n",
    "\n",
    "    Adjusted for use in Colab (see https://github.com/skoch9/meshplot/issues/29)\n",
    "    \"\"\"\n",
    "    ploot = None\n",
    "    points = points if points is not None else []\n",
    "    meshes = meshes if meshes is not None else []\n",
    "    lines = lines if lines is not None else []\n",
    "    mp.offline = True\n",
    "    for p in points:\n",
    "        if ploot is None:\n",
    "            ploot = mp.plot(**p)\n",
    "        else:\n",
    "            ploot.add_points(**p)\n",
    "    for m in meshes:\n",
    "        if 'mesh' in m.keys():\n",
    "            m['v'] = m['mesh'].vertices\n",
    "            m['f'] = m['mesh'].faces\n",
    "            del m['mesh']\n",
    "        elif 'voxel' in m.keys():\n",
    "            m['v'] = m['voxel'].as_boxes().vertices\n",
    "            m['f'] = m['voxel'].as_boxes().faces\n",
    "            del m['voxel']\n",
    "        if ploot is None:\n",
    "            ploot = mp.plot(**m)\n",
    "        else:\n",
    "            ploot.add_mesh(**m)\n",
    "    for l in lines:\n",
    "        if ploot is None:\n",
    "            ploot = mp.plot(**l)\n",
    "        else:\n",
    "            _ = ploot.add_lines(**l)\n",
    "    return ploot.to_html(imports=True, html_frame=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cmap = plt.get_cmap('jet') \n",
    "cube_colors_random = True\n",
    "if cube_colors_random:\n",
    "    random_colors = np.array([cmap(i)[:3] for i in np.linspace(0, 1, 12)])  # each triangle a different color\n",
    "else:\n",
    "    random_colors = np.array([cmap(0.5)[:3] for i in range(12)])  # uniform color"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Egocentric\n",
    "\n",
    "The egocentric pose is the orientation of the object w.r.t. the camera coordinate system. \n",
    "In the visualization below, our object passes the camera (from right to left) moving in a straight line.\n",
    "Its rotation w.r.t. to the camera does not change, however, as mentioned before, the appearance changes: On the right we have more information from the front-facing side and on the left more from the back-facing one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Settings\n",
    "egocentric_boxes = []\n",
    "extent = 0.25\n",
    "use_random_rotation = False\n",
    "if use_random_rotation:\n",
    "    rotation = R.random().as_matrix()   # define a random rotation\n",
    "else:\n",
    "    rotation = np.eye(3)\n",
    "\n",
    "# Create scene objects\n",
    "for i in [-2, -1, 0, 1, 2]:\n",
    "    translation = np.array([i, 0, -1])  # create different translations in `X` direction\n",
    "    transform = compose(translation, rotation, np.ones((3,)))\n",
    "\n",
    "    box = trimesh.creation.box(\n",
    "        extents=[extent]*3,\n",
    "        transform=transform\n",
    "    )  # create box with given extent and transformation\n",
    "    egocentric_boxes.append({\"v\": np.asarray(box.vertices), \"f\": np.asarray(box.faces), \"c\": random_colors})\n",
    "\n",
    "# Visualize\n",
    "html = visualize3d(\n",
    "    meshes=egocentric_boxes, \n",
    "    points=[{'v': np.array([[0, 0, 0]]), \"shading\": {\"point_size\": 0.4}}]  # camera at position (0, 0, 0)\n",
    ")\n",
    "display(HTML(html))\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Allocentric\n",
    "\n",
    "The allocentric pose refers to the orientation w.r.t. other objects and is sometimes also called orientation or observation angle.\n",
    "We visualize objects that all have the same allocentric rotation, i.e. the same rotation relative to a ray going through the object's center.\n",
    "\n",
    "See [3D-RCNN: Instance-level 3D Object Reconstruction via Render-and-Compare](https://openaccess.thecvf.com/content_cvpr_2018/papers/Kundu_3D-RCNN_Instance-Level_3D_CVPR_2018_paper.pdf) Section 4.2, with following notation changes\n",
    "- `R_egocentric` ~ `R`: Egocentric rotation\n",
    "- `R_allocentric` ~ `R_v`: Allocentric rotation\n",
    "- `R_pa_align` ~ `R_c`: Alignment of principal axis and the ray from the camera center through the object center\n",
    "\n",
    "Other implementations [[1]](https://github.com/siyeonkim33/fetch_manipulation/blob/324d014430af8807e47ca5057743ef2fbc1160b4/src/pose_cnn/lib/utils/se3.py#L32), [[2]](https://github.com/tiantianxuabc/dd3d-supplement/blob/0ad20f5475c45ca5f9009418cef224c009012276/tridet/utils/geometry.py), [[3]](https://github.com/KhiemPhi/Pose-Trans/blob/8ddfa8ce648802699644fe8f4f68062b893e9475/core/utils/utils.py#L97)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Settings\n",
    "if use_random_rotation:\n",
    "    R_allocentric = R.random()  \n",
    "else:\n",
    "    R_allocentric = R.from_matrix(np.eye(3))  # easier to see with eye\n",
    "# Compute object center positions in a circle\n",
    "length = 2\n",
    "eps = 0.1\n",
    "angle = np.pi * np.linspace(-1 + eps, 0 - eps, 5)\n",
    "x = np.cos(angle) * length\n",
    "y = np.zeros_like(x)\n",
    "z = np.sin(angle) * length\n",
    "object_centers = np.stack((x, y, z)).T\n",
    "allocentric_boxes = []\n",
    "\n",
    "\n",
    "# Create scene objects\n",
    "for i in range(5):\n",
    "    principal_axis = np.array([0, 0, 1]).reshape(-1, 3)\n",
    "    look_at = object_centers[i, ...]\n",
    "    R_pa_align, _ = R.align_vectors(look_at.reshape(-1, 3), principal_axis)\n",
    "    R_egocentric = R_pa_align * R_allocentric\n",
    "    transform = compose(look_at, R_egocentric.as_matrix(), np.ones((3,)))\n",
    "\n",
    "    box = trimesh.creation.box(\n",
    "        extents=[extent]*3,\n",
    "        transform=transform\n",
    "    )\n",
    "    box_plot_data = {\"v\": np.asarray(box.vertices), \"f\": np.asarray(box.faces), \"c\": random_colors}\n",
    "    allocentric_boxes.append(box_plot_data)\n",
    "\n",
    "# Visualize\n",
    "html = visualize3d(\n",
    "    meshes=allocentric_boxes, \n",
    "    points=[{'v': np.array([[0, 0, 0]]), \"shading\": {\"point_size\": 0.4}}]\n",
    ")\n",
    "display(HTML(html))\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note, that the conitnuous 6D pose estimation from [On the Continuity of Rotation Representations in Neural Networks](https://arxiv.org/abs/1812.07035) might be interesting as well. It is implemented in [PyTorch3D](https://pytorch3d.org/) as [matrix_to_rotation_6d](https://pytorch3d.readthedocs.io/en/latest/modules/transforms.html#pytorch3d.transforms.matrix_to_rotation_6d) and [rotation_6d_to_matrix](https://pytorch3d.readthedocs.io/en/latest/modules/transforms.html#pytorch3d.transforms.rotation_6d_to_matrix)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "3bd6af98ed0babec2915c9769a370431fb7f332bda9b24458d57257474d2ce5f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
